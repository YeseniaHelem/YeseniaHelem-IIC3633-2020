**Performance of Recommender Algorithms on Top-N Recommendation Tasks**

El trabajo [1] explora el rendimiento de diferentes métodos de sistemas recomendadación para diferentes tareas. Describe cuan importante es seleccionar el método, y cómo saber adaptarlo para otras tareas. Para el caso de recomendaciones entorno a Top-N, un problema recurrente es cuando se mide el rendimiento, como por ejemplo la minimización de métricas de error, como es el caso de RMSE ó MAE. El trabajo evalúa el desempeño de varios algoritmos, el cual lo hace mediante métricas de precision y recall, además de demostrar que no existe una relación entre las métricas de error y las métricas de precision. Además de proponer diferentes versiones a los algoritmos existentes.

Me llamó la atención la simplicidad de la variante del modelo de factor latente que presentan (PureSVD: construido bajo SVD), el trabajo representa a los usuario como una combinación de características de ítems. Otro aspecto que me llamó la atención es la metodología de short-head y long-tail, es bastante interesante; donde  es short-head se filtran los elementos más populares y en long-tail los elementos con menor calificación. En el fondo los autores trabajaron muy bien el tema de eficiencia, y es donde también se destaca el el buen rendimiento en modelos online y offline, en cuanto a la interacción con el usuario. 

Si bien obtienen buenos resultados en los experimentos que hicieron, además de tener una buena metodología, el hecho de trabajar con 1000 ítems combinados aleatoriamente, en donde suponen que la mayoría de ellos no son de interés de usuario. En ese sentido, ¿esto tendrá significancia al momento de evaluar la significancia de lo cada ítem combinado?, ya que en su supuesto esta que ‘no son de interés del usuario’. Pienso que elaborar un procedimiento a priori, podría traer mejores resultados en temas de adaptabilidad hacia otros set de datos, donde se elijan las muestras de en base a un conjunto de probabilidades, en vez de ser aleatoria.

El trabajo muestra cómo trabajó con las bases de datos completas, tanto para los usuario como para los ítems, en ese sentido la métrica de evaluación que elaboraron es más sólida. Finalmente, muestran un ventana amplia hacia una pre evaluación sobre las métricas de evaluación que se hacen a la hora de medir el desempeño de un algoritmo.

[1] Cremonesi, P., Koren, Y., & Turrin, R. (2010). Performance of recommender algorithms on top-n recommendation tasks. In Proceedings of the fourth ACM conference on Recommender systems (pp. 39-46). ACM.
